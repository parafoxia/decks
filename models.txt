
    0001 - 0005 (only changes epochs and batch size)
    -----------

def build_net(ds):
    enc = text_encoder(ds, 2500)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=64,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
            tf.keras.layers.Dense(64, activation="relu"),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer="adam",
        metrics=["accuracy"],
    )
    return model


    0006
    ----

# Dropout layer (https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf, p. 1938)

def build_net(ds):
    enc = text_encoder(ds, 2500)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=64,
                mask_zero=True,
            ),
            tf.keras.layer.Dropout(0.8),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
            tf.keras.layer.Dropout(0.7),
            tf.keras.layers.Dense(64, activation="relu"),
            tf.keras.layer.Dropout(0.6),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer="adam",
        metrics=["accuracy"],
    )
    return model


    0007
    ----

def build_net(ds):
    enc = text_encoder(ds, 2500)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=64,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
            tf.keras.layers.Dense(64, activation="relu"),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    0008
    ----

def build_net(ds):
    enc = text_encoder(ds, 2500)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=32,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),
            tf.keras.layers.Dense(32, activation="relu"),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    0009
    ----

def build_net(ds):
    enc = text_encoder(ds, 10_000)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=128,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
            tf.keras.layers.Dense(64, activation="relu"),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    0012
    ----

def build_net(ds):
    hl1, hl2 = calculate_nodes_for(ds, 6)
    print(f"Using \33[1m{hl1}:{hl2}\33[0m for hidden node counts")

    enc = text_encoder(ds, None)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=hl1,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hl1)),
            tf.keras.layers.Dense(hl2, activation="relu"),
            tf.keras.layers.Dense(6, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    0013
    ----

    0012 + weights

    0014
    ----

    0013 + learning rate = 2e-05

    0015
    ----

    0007 with new standardiser

    1001
    ----

def build_net(ds):
    enc = text_encoder(ds, 2_500)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=64,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),
            tf.keras.layers.Dense(64, activation="relu"),
            tf.keras.layers.Dense(28, activation="sigmoid"),
        ]
    )
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["accuracy"],
    )
    return model

    1002
    ----

class Model(tf.keras.Model):
    def __init__(self, ds):
        super().__init__()

        self.encoder = text_encoder(ds, 2_500)
        self.embedding = tf.keras.layers.Embedding(input_dim=len(self.encoder.get_vocabulary()), output_dim=64, mask_zero=True)

        self.b1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.b2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.b3 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))

        self.d1 = tf.keras.layers.Dense(64, activation="relu")
        self.d2 = tf.keras.layers.Dense(64, activation="relu")
        self.d3 = tf.keras.layers.Dense(64, activation="relu")

        self.out = tf.keras.layers.Dense(28, activation="sigmoid")

    def call(self, inputs):
        x = self.encoder(inputs)
        x = self.embedding(x)
        x = self.b1(x)
        x = self.b2(x)
        x = self.b3(x)
        x = self.d1(x)
        x = self.d2(x)
        x = self.d3(x)
        return self.out(x)

def build_net(ds):
    model = Model(ds)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    1003
    ----

-

    1004
    ----

class Model(tf.keras.Model):
    def __init__(self, ds):
        super().__init__()

        self.encoder = text_encoder(ds, 2_500)
        self.embedding = tf.keras.layers.Embedding(input_dim=len(self.encoder.get_vocabulary()), output_dim=64, mask_zero=True)

        self.b1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.b2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.b3 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))

        self.d1 = tf.keras.layers.Dense(64, activation="relu")
        self.d2 = tf.keras.layers.Dense(64, activation="relu")
        self.d3 = tf.keras.layers.Dense(64, activation="relu")

        self.out = tf.keras.layers.Dense(28, activation="sigmoid")

    def call(self, inputs):
        x = self.encoder(inputs)
        x = self.embedding(x)
        x = self.b1(x)
        x = self.b2(x)
        x = self.b3(x)
        x = self.d1(x)
        x = self.d2(x)
        x = self.d3(x)
        return self.out(x)


def build_net(ds):
    model = Model(ds)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-4),
        metrics=["accuracy"],
    )
    return model

    1005
    ----

class Model(tf.keras.Model):
    def __init__(self, ds):
        super().__init__()

        self.encoder = text_encoder(ds, 2_500)
        self.embedding = tf.keras.layers.Embedding(input_dim=len(self.encoder.get_vocabulary()), output_dim=64, mask_zero=True)

        self.bidirectional_1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.bidirectional_2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_state=True, return_sequences=True))
        self.bidirectional_3 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64))

        self.dense_1 = tf.keras.layers.Dense(64, activation="relu")
        self.dense_2 = tf.keras.layers.Dense(64, activation="relu")
        self.dense_3 = tf.keras.layers.Dense(64, activation="relu")

        self.out = tf.keras.layers.Dense(28, activation="sigmoid")

    def call(self, inputs):
        x = self.encoder(inputs)
        x = self.embedding(x)
        x = self.bidirectional_1(x)
        x = self.bidirectional_2(x)
        x = self.bidirectional_3(x)
        x = self.dense_1(x)
        x = self.dense_2(x)
        x = self.dense_3(x)
        return self.out(x)

def build_net(ds):
    model = Model(ds)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["accuracy"],
    )
    return model

    1006
    ----

def build_net(ds, n_samples):
    hl1, hl2, outputs = calculate_nodes_for(ds, n_samples)
    enc = text_encoder(ds, None)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=hl1,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(hl1)),
            tf.keras.layers.Dense(hl2, activation="relu"),
            tf.keras.layers.Dense(outputs, activation="sigmoid"),
        ]
    )
    model.compile(
        loss="binary_crossentropy",
        # optimizer=tf.keras.optimizers.Adam(1e-4),
        optimizer="adam",
        metrics=["accuracy"],
    )
    return model

    2001 (90:54)
    ------------

    * Huang's theorem (100%)

def build_net(ds):
    outputs = 6
    h1, h2 = calculate_hidden_nodes(len(ds), outputs)
    print(f"Using {h1}:{h2} for hidden layers")

    enc = text_encoder(ds, None)
    model = tf.keras.Sequential(
        [
            enc,
            tf.keras.layers.Embedding(
                input_dim=len(enc.get_vocabulary()),
                output_dim=h1,
                mask_zero=True,
            ),
            tf.keras.layers.Bidirectional(tf.keras.layers.GRU(h1)),
            tf.keras.layers.Dense(h2, activation="relu"),
            tf.keras.layers.Dense(outputs, activation="softmax"),
        ]
    )
    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer="adam",
        metrics=[
            "accuracy",
            *[metrics.PrecisionForClass(x) for x in range(outputs)],
            *[metrics.RecallForClass(x) for x in range(outputs)],
            *[metrics.MccForClass(x) for x in range(outputs)],
        ],
    )
    return model

    2002 (67:40)
    ------------

    2001 w/ Huang's theorem (75%)

    2003 (45:27)
    ------------

    2001 w/ Huang's theorem (50%)
